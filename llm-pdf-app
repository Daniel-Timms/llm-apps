import streamlit as st
import os
import datetime
import time
import tempfile
import openai
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

import streamlit as st

# Custom CSS
st.markdown("""
<style>
/* Change the background of the chat bubble. Modify this according to your needs. */
.stChatMessage {
    background-color: #f5fbf6 !important; /* Light gray for demonstration */
}

</style>
""", unsafe_allow_html=True)

st.markdown("""
<style>
    /* Modify the color of h1 elements (main title) */
    h1 {
        color: #00ab4e;  /* This is a sample color - replace with your preferred color */
    }
</style>
""", unsafe_allow_html=True)



# Setup OpenAI API Key
os.environ['OPENAI_API_KEY'] = 'please-dont-steal'
llm_name = "gpt-4"

st.title('PDF Chatbot :rocket:')

# PDF Uploader
pdf_file = st.file_uploader("Choose a PDF file", type=['pdf'])
if pdf_file:
    # Save uploaded PDF to a temporary location
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(pdf_file.getvalue())
        temp_path = tmp.name

    # Load PDF data using the temporary file path
    loaders = [PyPDFLoader(temp_path)]
    docs = []
    for loader in loaders:
        docs.extend(loader.load())

    # Split data
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)
    splits = text_splitter.split_documents(docs)

    # Embeddings and vectorstore
    embedding = OpenAIEmbeddings()
    vectordb = FAISS.from_documents(splits, embedding)

    # Chat model
    llm = ChatOpenAI(model_name=llm_name, temperature=0)
    template = """
    Use the following pieces of context to answer the question at the end. 
    {context}
    Question: {question}
    Helpful Answer:
    """
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"], template=template)

    # Memory
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    
    # Conversational Retrieval Chain
    retriever = vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)

    # Initialize or retrieve conversation history from Streamlit's session state
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # Define a function to get the default avatar based on the role
    def get_default_avatar(role):
        if role == "user":
            return "üßë‚Äçüíª"
        elif role == "assistant":
            return "ü§ñ"
        else:
            return None

    # Display the chat history dynamically
    for message in st.session_state.messages:
        # If avatar key doesn't exist in the message, get the default avatar
        avatar = message.get("avatar", get_default_avatar(message["role"]))
        with st.chat_message(message["role"], avatar=avatar):
            st.markdown(message["content"])



    # Initialize session state if not already
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # User input
    question = st.chat_input("Enter your query:")

    if question:
        # Append the user's message with the specified avatar to the messages list
        st.session_state.messages.append({"role": "user", "avatar": "üßë‚Äçüíª", "content": question})
        
        with st.chat_message("user", avatar="üßë‚Äçüíª"):
            st.markdown(question)
        
        # Display the spinner while fetching the response
        with st.spinner(text=""):
            # Fetch response using the qa function (replace this with your function)
            result = qa({"question": question})
            response_content = result['answer']
        
        # Placeholder to simulate typing
        typing_placeholder = st.empty()

        displayed_content = ""
        for word in response_content.split():
            displayed_content += word + " "
            typing_placeholder.markdown(displayed_content + "‚ñå")  # The ‚ñå character gives the illusion of a cursor
            time.sleep(0.1)  # Adjust this for desired speed

        typing_placeholder.empty()  # Clear the placeholder

        # Display the full response in a single chat bubble
        with st.chat_message("assistant", avatar="ü§ñ"):
            st.markdown(response_content)

        # Append the assistant's message with the specified avatar to the messages list
        st.session_state.messages.append({"role": "assistant", "avatar": "ü§ñ", "content": response_content})


# http://localhost:8502/